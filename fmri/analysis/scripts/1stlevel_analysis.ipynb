{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180828-02:55:24,381 duecredit ERROR:\n",
      "\t Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from inspect import currentframe, getframeinfo\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from nipype.interfaces.base import Bunch\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "\n",
    "from utils.event_utils import process_confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments\n",
    "These are not needed for the jupyter notebook, but are used after conversion to a script for production\n",
    "\n",
    "- conversion command:\n",
    "  - jupyter nbconvert --to script --execute 1stlevel_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Example BIDS App entrypoint script.')\n",
    "parser.add_argument('-data_dir', default='/data')\n",
    "parser.add_argument('-derivatives_dir', default=None)\n",
    "parser.add_argument('-working_dir', default=None)\n",
    "parser.add_argument('--subjids', nargs=\"+\")\n",
    "parser.add_argument('--sessions', nargs=\"+\", default=[1])\n",
    "parser.add_argument('--runs', nargs=\"+\", default=[1,2])\n",
    "parser.add_argument('--n_procs', default=16, type=int)\n",
    "if '-derivatives_dir' in sys.argv or '-h' in sys.argv:\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args([])\n",
    "    args.data_dir = '/data'\n",
    "    args.derivatives_dir = '/data/derivatives'\n",
    "    args.subjids = ['s0324']\n",
    "    args.n_procs=4\n",
    "    owd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get current directory to pass to function nodes\n",
    "filename = getframeinfo(currentframe()).filename\n",
    "current_directory = str(Path(filename).resolve().parent)\n",
    "\n",
    "#### Experiment Variables\n",
    "derivatives_dir = args.derivatives_dir\n",
    "fmriprep_dir = join(derivatives_dir, 'fmriprep', 'fmriprep')\n",
    "data_dir = args.data_dir\n",
    "first_level_dir = join(derivatives_dir,'1stlevel')\n",
    "if args.working_dir is None:\n",
    "    working_dir = join(derivatives_dir, '1stlevel_workingdir')\n",
    "else:\n",
    "    working_dir = join(args.working_dir, '1stlevel_workingdir')\n",
    "n_procs = args.n_procs\n",
    "# TR of functional images\n",
    "TR = .71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Subject(s): ['s0324']\n",
      ", derivatives_dir: /data/derivatives\n",
      ", data_dir: /data\n",
      "*******************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('*'*79)\n",
    "print('Subject(s): %s\\n, derivatives_dir: %s\\n, data_dir: %s' % \n",
    "     (args.subjids, derivatives_dir, data_dir))\n",
    "print('*'*79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_regressors(subject_id, session, run, base_dir):\n",
    "    from glob import glob\n",
    "    import pandas as pd\n",
    "    # strip \"sub\" from beginning of subject_id if provided\n",
    "    subject_id = subject_id.replace('sub-','')\n",
    "    ## Get the Confounds File (output of fmriprep)\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    confounds_file = glob(join(fmriprep_dir,\n",
    "                               'sub-%s' % subject_id,\n",
    "                               'ses-%s' % session, 'func',\n",
    "                               '*run-%s*confounds.tsv' % run))\n",
    "    ## Get the Events File if it exists\n",
    "    # Read the TSV file and convert to pandas dataframe\n",
    "    event_file = glob(join(data_dir, 'fmriprep', 'fmriprep',\n",
    "                           'sub-%s' % subject_id,\n",
    "                           'ses-%s' % session, 'func',\n",
    "                           '*run-%s*events.tsv' % run))  \n",
    "    assert len(event_file) == 1 and len(confounds_file) == 1\n",
    "    # set up events file\n",
    "    event_file = event_file[0]\n",
    "    events_df = pd.read_csv(event_file,sep = '\\t')\n",
    "    regressors, regressor_names = process_confounds(confounds_file[0])\n",
    "    return events_df, regressors, regressor_names\n",
    "\n",
    "def get_subject_info(events_df, regressors, regressor_names): \n",
    "    # subset events_df\n",
    "    events_subset = events_df.query('type==\"response\"')\n",
    "    EV_dict = {'conditions': ['motor'],\n",
    "               'onsets': [list(events_subset.onset)],\n",
    "               'durations': [list(events_subset.duration)],\n",
    "               'amplitudes': [[1]]}\n",
    "    contrasts = [['motor', 'T', ['motor'], [1]]]\n",
    "    subject_info = Bunch(conditions=EV_dict['conditions'],\n",
    "                        onsets=EV_dict['onsets'],\n",
    "                        durations=EV_dict['durations'],\n",
    "                        amplitudes=EV_dict['amplitudes'],\n",
    "                        tmod=None,\n",
    "                        pmod=None,\n",
    "                        regressor_names=regressor_names,\n",
    "                        regressors=regressors.T.tolist())\n",
    "    return subject_info, contrasts\n",
    "\n",
    "\n",
    "# Create the function Nodes\n",
    "getevents = Node(Function(input_names=['subject_id', 'session', 'run', 'data_dir'],\n",
    "                          output_names=['events_df', 'regressors', 'regressor_names'],\n",
    "                         function=get_events_regressors),\n",
    "                    name='getevents')\n",
    "\n",
    "getsubjectinfo = Node(Function(input_names=['events_df', 'regressors', 'regressor_names'],\n",
    "                             output_names=['subject_info', 'contrasts'],\n",
    "                         function=get_subject_info),\n",
    "                    name='getsubjectinfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ridiculous regexp substitution to get files just right\\n# link to ridiculousness: https://regex101.com/r/ljS5zK/3\\nmatch_str = \"(?P<sub>s[0-9]+)\\\\/(?P<task>[A-Za-z1-9_]+)_(?P<model>model-[a-z]+)_(?P<submodel>wf-[a-z]+)\\\\/(s[0-9]+/|)\"\\nreplace_str = \"\\\\g<sub>/\\\\g<task>/\\\\g<model>/\\\\g<submodel>/\"\\nregexp_substitutions = [(match_str, replace_str)]\\ndatasink.inputs.regexp_substitutions = regexp_substitutions\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infosource = Node(IdentityInterface(fields=['subject_id',\n",
    "                                            'session',\n",
    "                                            'run',\n",
    "                                           'data_dir']),\n",
    "                  name=\"infosource\")\n",
    "infosource.inputs.data_dir = data_dir\n",
    "infosource.iterables = [('subject_id', args.subjids),\n",
    "                        ('session', args.sessions),\n",
    "                        ('run', args.runs)]\n",
    "\n",
    "# SelectFiles - to grab the data (alternative to DataGrabber)\n",
    "templates = {'func': join('sub-{subject_id}','ses-{session}','func',\n",
    "                         '*run-{run}*T1w_preproc.nii.gz'),\n",
    "             'mask': join('sub-{subject_id}','ses-{session}','func',\n",
    "                          '*run-{run}*T1w_brainmask.nii.gz')}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory=fmriprep_dir,\n",
    "                               sort_filelist=True),\n",
    "                   name='selectFiles')\n",
    "    \n",
    "masker = Node(fsl.maths.ApplyMask(), name='masker')\n",
    "\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory=first_level_dir), name=\"datasink\")\n",
    "# Use the following DataSink output substitutions\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('fstat', 'FSTST'),\n",
    "                ('run0.mat', 'designfile.mat')]\n",
    "\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\"\"\"\n",
    "# ridiculous regexp substitution to get files just right\n",
    "# link to ridiculousness: https://regex101.com/r/ljS5zK/3\n",
    "match_str = \"(?P<sub>s[0-9]+)\\/(?P<task>[A-Za-z1-9_]+)_(?P<model>model-[a-z]+)_(?P<submodel>wf-[a-z]+)\\/(s[0-9]+/|)\"\n",
    "replace_str = \"\\g<sub>/\\g<task>/\\g<model>/\\g<submodel>/\"\n",
    "regexp_substitutions = [(match_str, replace_str)]\n",
    "datasink.inputs.regexp_substitutions = regexp_substitutions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpecifyModel - Generates FSL-specific Model\n",
    "modelspec = Node(SpecifyModel(input_units='secs',\n",
    "                              time_repetition=TR,\n",
    "                              high_pass_filter_cutoff=80),\n",
    "                 name=\"modelspec\")\n",
    "# Level1Design - Creates FSL config file \n",
    "level1design = Node(fsl.Level1Design(bases={'dgamma':{'derivs': True}},\n",
    "                                     interscan_interval=TR,\n",
    "                                     model_serial_correlations=True),\n",
    "                        name=\"level1design\")\n",
    "# FEATmodel generates an FSL design matrix\n",
    "level1model = Node(fsl.FEATModel(), name=\"FEATModel\")\n",
    "\n",
    "# FILMGLs\n",
    "# smooth_autocorr, check default, use FSL default\n",
    "filmgls = Node(fsl.FILMGLS(threshold=1000), name=\"GLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nselect_out = selectfiles.run()\\n\\nmasker.inputs.in_file = select_out.outputs.func\\nmasker.inputs.mask_file = select_out.outputs.mask\\nmask_out = masker.run()\\n\\nmodelspec.inputs.functional_runs = mask_out.outputs.out_file\\nmodelspec_out = modelspec.run()\\n\\nlevel1design.inputs.session_info = modelspec_out.outputs.session_info\\nlevel1design_out = level1design.run()\\n\\nlevel1model.inputs.ev_files = level1design_out.outputs.ev_files\\nlevel1model.inputs.fsf_file = level1design_out.outputs.fsf_files\\nlevel1model_out = level1model.run()\\n\\nfilmgls.inputs.design_file = level1model_out.outputs.design_file\\nfilmgls.inputs.tcon_file = level1model_out.outputs.con_file\\nfilmgls.inputs.fcon_file = level1model_out.outputs.fcon_file\\nfilmgls.inputs.in_file = mask_out.outputs.out_file\\nfilm_out = filmgls.run()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "select_out = selectfiles.run()\n",
    "\n",
    "masker.inputs.in_file = select_out.outputs.func\n",
    "masker.inputs.mask_file = select_out.outputs.mask\n",
    "mask_out = masker.run()\n",
    "\n",
    "modelspec.inputs.functional_runs = mask_out.outputs.out_file\n",
    "modelspec_out = modelspec.run()\n",
    "\n",
    "level1design.inputs.session_info = modelspec_out.outputs.session_info\n",
    "level1design_out = level1design.run()\n",
    "\n",
    "level1model.inputs.ev_files = level1design_out.outputs.ev_files\n",
    "level1model.inputs.fsf_file = level1design_out.outputs.fsf_files\n",
    "level1model_out = level1model.run()\n",
    "\n",
    "filmgls.inputs.design_file = level1model_out.outputs.design_file\n",
    "filmgls.inputs.tcon_file = level1model_out.outputs.con_file\n",
    "filmgls.inputs.fcon_file = level1model_out.outputs.fcon_file\n",
    "filmgls.inputs.in_file = mask_out.outputs.out_file\n",
    "film_out = filmgls.run()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desmtx=np.loadtxt(level1model_out.outputs.design_file,skiprows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of the 1st-level analysis workflow\n",
    "l1analysis = Workflow(name='l1analysis')\n",
    "l1analysis.base_dir = working_dir\n",
    "l1analysis.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                               ('session', 'session'),\n",
    "                                               ('run', 'run')]),\n",
    "                    (infosource, getevents, [('subject_id', 'subject_id'),\n",
    "                                             ('session', 'session'),\n",
    "                                             ('run', 'run')]),\n",
    "                    (infosource, getevents, [('data_dir', 'data_dir')]),\n",
    "                    (getevents, getsubjectinfo, [('events_df', 'events_df'),\n",
    "                              ('regressors', 'regressors'), \n",
    "                              ('subjectinfo', 'subjectinfo')]),\n",
    "                    (selectfiles, masker, [('func','in_file'), ('mask', 'mask_file')]),\n",
    "                    (masker, modelspec, [('out_file', 'functional_runs')]),\n",
    "                    (getsubjectinfo, modelspec, [('subject_info', 'subject_info')]),\n",
    "                    (modelspec, level1design, [('session_info','session_info')]),\n",
    "                    (getsubjectinfo, level1design, [('contrasts', 'contrasts')]),\n",
    "                    (level1design, level1model, [('ev_files', 'ev_files'),\n",
    "                                             ('fsf_files','fsf_file')]),\n",
    "                    (infosource, datasink, [('subject_id', 'container')]),\n",
    "                    (level1model, datasink, [('design_file', 'FEAT.@design_file')]),\n",
    "                    (masker, filmgls, [('out_file', 'in_file')]),\n",
    "                    (level1model, filmgls, [('design_file', 'design_file'),\n",
    "                                        ('con_file', 'tcon_file'),\n",
    "                                        ('fcon_file', 'fcon_file')]),\n",
    "                    (filmgls, datasink, [('copes', 'filmgls.@copes'),\n",
    "                                     ('zstats', 'filmgls.@Z'),\n",
    "                                     ('fstats', 'filmgls.@F'),\n",
    "                                     ('tstats','filmgls.@T'),\n",
    "                                     ('param_estimates','filmgls.@param_estimates'),\n",
    "                                     ('residual4d', 'filmgls.@residual4d'),\n",
    "                                     ('sigmasquareds', 'filmgls.@sigmasquareds')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180828-02:56:47,214 workflow INFO:\n",
      "\t Workflow l1analysis settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180828-02:56:47,443 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180828-02:56:47,447 workflow INFO:\n",
      "\t Currently running 0 tasks, and 4 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180828-02:56:47,464 workflow INFO:\n",
      "\t Executing node l1analysis.selectFiles in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/selectFiles\n",
      "180828-02:56:47,482 workflow INFO:\n",
      "\t Executing node l1analysis.getevents in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/getevents\n",
      "180828-02:56:47,501 workflow INFO:\n",
      "\t Executing node l1analysis.selectFiles in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/selectFiles\n",
      "180828-02:56:47,524 workflow INFO:\n",
      "\t Executing node l1analysis.getevents in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/getevents\n",
      "180828-02:56:47,692 workflow INFO:\n",
      "\t Running node \"selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180828-02:56:47,715 workflow INFO:\n",
      "\t Running node \"getevents\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180828-02:56:47,837 workflow INFO:\n",
      "\t Running node \"selectFiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180828-02:56:47,853 workflow INFO:\n",
      "\t Running node \"getevents\" (\"nipype.interfaces.utility.wrappers.Function\").\n",
      "180828-02:56:49,525 workflow INFO:\n",
      "\t [Job finished] jobname: selectFiles.a1 jobid: 0\n",
      "180828-02:56:49,528 workflow ERROR:\n",
      "\t Node getevents.a1 failed to run on host 001e46a0e266.\n",
      "180828-02:56:49,532 workflow ERROR:\n",
      "\t Saving crash info to /scripts/scripts/crash-20180828-025649-root-getevents.a1-782dc3f7-6efe-409e-b153-037417779f07.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 137, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 7, in get_events_regressors\n",
      "NameError: name 'glob' is not defined\n",
      "\n",
      "180828-02:56:49,536 workflow INFO:\n",
      "\t [Job finished] jobname: selectFiles.a0 jobid: 9\n",
      "180828-02:56:49,539 workflow ERROR:\n",
      "\t Node getevents.a0 failed to run on host 001e46a0e266.\n",
      "180828-02:56:49,540 workflow ERROR:\n",
      "\t Saving crash info to /scripts/scripts/crash-20180828-025649-root-getevents.a0-9c7a6b98-8c00-4350-a6bb-c434d8db5b37.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 137, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 7, in get_events_regressors\n",
      "NameError: name 'glob' is not defined\n",
      "\n",
      "180828-02:56:49,546 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180828-02:56:49,586 workflow INFO:\n",
      "\t Executing node l1analysis.masker in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/masker\n",
      "180828-02:56:49,620 workflow INFO:\n",
      "\t Executing node l1analysis.masker in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/masker\n",
      "180828-02:56:49,734 workflow INFO:\n",
      "\t Running node \"masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /data/derivatives/fmriprep/fmriprep/sub-s0324/ses-1/func/sub-s0324_ses-1_task-CuedContext_run-2_bold_space-T1w_preproc.nii.gz -mas /data/derivatives/fmriprep/fmriprep/sub-s0324/ses-1/func/sub-s0324_ses-1_task-CuedContext_run-2_bold_space-T1w_brainmask.nii.gz /data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-2_bold_space-T1w_preproc_masked.nii.gz.\n",
      "180828-02:56:49,775 workflow INFO:\n",
      "\t Running node \"masker\" (\"nipype.interfaces.fsl.maths.ApplyMask\"), a CommandLine Interface with command:\n",
      "fslmaths /data/derivatives/fmriprep/fmriprep/sub-s0324/ses-1/func/sub-s0324_ses-1_task-CuedContext_run-1_bold_space-T1w_preproc.nii.gz -mas /data/derivatives/fmriprep/fmriprep/sub-s0324/ses-1/func/sub-s0324_ses-1_task-CuedContext_run-1_bold_space-T1w_brainmask.nii.gz /data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-1_bold_space-T1w_preproc_masked.nii.gz.\n",
      "180828-02:56:51,621 workflow INFO:\n",
      "\t Currently running 2 tasks, and 0 jobs ready. Free memory (GB): 10.05/10.45, Free processors: 2/4\n",
      "180828-02:59:21,936 workflow INFO:\n",
      "\t [Job finished] jobname: masker.a1 jobid: 1\n",
      "180828-02:59:21,939 workflow INFO:\n",
      "\t Currently running 1 tasks, and 1 jobs ready. Free memory (GB): 10.25/10.45, Free processors: 3/4\n",
      "180828-02:59:21,974 workflow INFO:\n",
      "\t Executing node l1analysis.GLS in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/GLS\n",
      "180828-02:59:22,97 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --in=/data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-2_bold_space-T1w_preproc_masked.nii.gz --thr=1000.000000.\n",
      "180828-02:59:23,972 workflow INFO:\n",
      "\t [Job finished] jobname: masker.a0 jobid: 10\n",
      "180828-02:59:23,975 workflow INFO:\n",
      "\t Currently running 1 tasks, and 1 jobs ready. Free memory (GB): 10.25/10.45, Free processors: 3/4\n",
      "180828-02:59:24,224 workflow INFO:\n",
      "\t Executing node l1analysis.GLS in dir: /data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/GLS\n",
      "180828-02:59:25,300 workflow INFO:\n",
      "\t Running node \"GLS\" (\"nipype.interfaces.fsl.model.FILMGLS\"), a CommandLine Interface with command:\n",
      "film_gls --rn=results --in=/data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-1_bold_space-T1w_preproc_masked.nii.gz --thr=1000.000000.\n",
      "180828-02:59:26,226 workflow INFO:\n",
      "\t Currently running 2 tasks, and 0 jobs ready. Free memory (GB): 10.05/10.45, Free processors: 2/4\n",
      "180828-03:00:52,384 workflow ERROR:\n",
      "\t Node GLS.a1 failed to run on host 001e46a0e266.\n",
      "180828-03:00:52,387 workflow ERROR:\n",
      "\t Saving crash info to /scripts/scripts/crash-20180828-030052-root-GLS.a1-2a691dcd-0288-42c0-81a7-faea025cf5f9.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1696, in _run_interface\n",
      "    self.raise_exception(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1639, in raise_exception\n",
      "    **runtime.dictcopy()))\n",
      "RuntimeError: Command:\n",
      "film_gls --rn=results --in=/data/derivatives/1stlevel_workingdir/l1analysis/_run_2_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-2_bold_space-T1w_preproc_masked.nii.gz --thr=1000.000000\n",
      "Standard output:\n",
      "Log directory is: results\n",
      "paradigm.getDesignMatrix().Nrows()=0\n",
      "paradigm.getDesignMatrix().Ncols()=0\n",
      "sizeTS=592\n",
      "numTS=112194\n",
      "Calculating residuals...\n",
      "Standard error:\n",
      "\n",
      "\n",
      "An exception has been thrown\n",
      "Logic error:- detected by Newmat: Maximum or minimum of null matrix\n",
      "\n",
      "Trace: pinv.\n",
      "\n",
      "Return code: 1\n",
      "\n",
      "180828-03:00:52,390 workflow INFO:\n",
      "\t Currently running 1 tasks, and 0 jobs ready. Free memory (GB): 10.25/10.45, Free processors: 3/4\n",
      "180828-03:00:58,402 workflow ERROR:\n",
      "\t Node GLS.a0 failed to run on host 001e46a0e266.\n",
      "180828-03:00:58,404 workflow ERROR:\n",
      "\t Saving crash info to /scripts/scripts/crash-20180828-030058-root-GLS.a0-c0dc4ee3-b31d-4030-ac1b-74f140d37a06.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1089, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1696, in _run_interface\n",
      "    self.raise_exception(runtime)\n",
      "  File \"/usr/local/miniconda/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1639, in raise_exception\n",
      "    **runtime.dictcopy()))\n",
      "RuntimeError: Command:\n",
      "film_gls --rn=results --in=/data/derivatives/1stlevel_workingdir/l1analysis/_run_1_session_1_subject_id_s0324/masker/sub-s0324_ses-1_task-CuedContext_run-1_bold_space-T1w_preproc_masked.nii.gz --thr=1000.000000\n",
      "Standard output:\n",
      "Log directory is: results\n",
      "paradigm.getDesignMatrix().Nrows()=0\n",
      "paradigm.getDesignMatrix().Ncols()=0\n",
      "sizeTS=592\n",
      "numTS=113941\n",
      "Calculating residuals...\n",
      "Standard error:\n",
      "\n",
      "\n",
      "An exception has been thrown\n",
      "Logic error:- detected by Newmat: Maximum or minimum of null matrix\n",
      "\n",
      "Trace: pinv.\n",
      "\n",
      "Return code: 1\n",
      "\n",
      "180828-03:00:58,410 workflow INFO:\n",
      "\t Currently running 0 tasks, and 0 jobs ready. Free memory (GB): 10.45/10.45, Free processors: 4/4\n",
      "180828-03:01:00,412 workflow INFO:\n",
      "\t ***********************************\n",
      "180828-03:01:00,413 workflow ERROR:\n",
      "\t could not run node: l1analysis.getevents.a1\n",
      "180828-03:01:00,414 workflow INFO:\n",
      "\t crashfile: /scripts/scripts/crash-20180828-025649-root-getevents.a1-782dc3f7-6efe-409e-b153-037417779f07.pklz\n",
      "180828-03:01:00,415 workflow ERROR:\n",
      "\t could not run node: l1analysis.getevents.a0\n",
      "180828-03:01:00,416 workflow INFO:\n",
      "\t crashfile: /scripts/scripts/crash-20180828-025649-root-getevents.a0-9c7a6b98-8c00-4350-a6bb-c434d8db5b37.pklz\n",
      "180828-03:01:00,417 workflow ERROR:\n",
      "\t could not run node: l1analysis.GLS.a1\n",
      "180828-03:01:00,417 workflow INFO:\n",
      "\t crashfile: /scripts/scripts/crash-20180828-030052-root-GLS.a1-2a691dcd-0288-42c0-81a7-faea025cf5f9.pklz\n",
      "180828-03:01:00,420 workflow ERROR:\n",
      "\t could not run node: l1analysis.GLS.a0\n",
      "180828-03:01:00,421 workflow INFO:\n",
      "\t crashfile: /scripts/scripts/crash-20180828-030058-root-GLS.a0-c0dc4ee3-b31d-4030-ac1b-74f140d37a06.pklz\n",
      "180828-03:01:00,421 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b1a9045850c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#l1analysis.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ml1analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_procs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# close any open resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "#l1analysis.run()\n",
    "l1analysis.run('MultiProc', plugin_args={'n_procs': n_procs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
