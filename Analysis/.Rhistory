curve(relative_probabilities(x,.1,.9),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("bottomright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
#graph the two distributions and their relative likelihood assuming a flat prior
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
#show relative likelihood including a recursive transition probability of .9
#dashed line shows the likelihood given that one is in state 2 (mu = mu2)
#dotted line shows likelihood given one is in state 1
relative_probabilities = function(x,prior1, prior2) {
dnorm(x,mean = mu1, sd = sd1)*prior1/
(dnorm(x,mean = mu2, sd = sd2)*prior2+
dnorm(x,mean = mu1, sd = sd1)*prior1)
}
curve(relative_probabilities(x,.5,.5),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(relative_probabilities(x,.9,.1),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(relative_probabilities(x,.1,.9),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
choice_probabilities = function(x,prior1, prior2, value1, value2) {
dnorm(x,mean = mu1, sd = sd1)*prior1*value1/
(dnorm(x,mean = mu2, sd = sd2)*prior2*value2+
dnorm(x,mean = mu1, sd = sd1)*prior1*value1)
}
curve(choice_probabilities(x,.5,.5,1,2),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(choice_probabilities(x,.9,.1,1,2),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(choice_probabilities(x,.1,.9,1,2),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
#graph the two distributions and their relative likelihood assuming a flat prior
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
#show relative likelihood including a recursive transition probability of .9
#dashed line shows the likelihood given that one is in state 2 (mu = mu2)
#dotted line shows likelihood given one is in state 1
relative_probabilities = function(x,prior1, prior2) {
dnorm(x,mean = mu1, sd = sd1)*prior1/
(dnorm(x,mean = mu2, sd = sd2)*prior2+
dnorm(x,mean = mu1, sd = sd1)*prior1)
}
curve(relative_probabilities(x,.5,.5),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(relative_probabilities(x,.9,.1),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(relative_probabilities(x,.1,.9),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
#incorporate utility, weight probabilities by maximal utility possible
choice_probabilities = function(x,prior1, prior2, value1, value2) {
dnorm(x,mean = mu1, sd = sd1)*prior1*value1/
(dnorm(x,mean = mu2, sd = sd2)*prior2*value2+
dnorm(x,mean = mu1, sd = sd1)*prior1*value1)
}
#graph the two distributions and their relative likelihood assuming a flat prior
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2*2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
curve(choice_probabilities(x,.5,.5,1,2),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(choice_probabilities(x,.9,.1,1,2),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(choice_probabilities(x,.1,.9,1,2),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2*2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,3))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,3))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,3))
par(new = T)
curve(dnorm(x,mean = mu1, sd = sd1)/
(dnorm(x,mean = mu1, sd = sd1)+dnorm(x,mean = mu2, sd = sd2)*2),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,3), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,3))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,3))
par(new = T)
curve(dnorm(x,mean = mu1, sd = sd1)/
(dnorm(x,mean = mu1, sd = sd1)+dnorm(x,mean = mu2, sd = sd2)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,3), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,3))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,3))
par(new = T)
curve(dnorm(x,mean = mu1, sd = sd1)/
(dnorm(x,mean = mu1, sd = sd1)+dnorm(x,mean = mu2, sd = sd2)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,3), col = 'red')
par(new = T)
curve(dnorm(x,mean = mu1, sd = sd1)/
(dnorm(x,mean = mu1, sd = sd1)+dnorm(x,mean = mu2, sd = sd2)*2),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,3), col = 'red')
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,3))
par(new = T)
curve(2*dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,3))
par(new = T)
curve(dnorm(x,mean = mu1, sd = sd1)/
(dnorm(x,mean = mu1, sd = sd1)+dnorm(x,mean = mu2, sd = sd2)*2),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,3), col = 'red')
#Without hypothesis priors one should change their policy based on whether the
#context is > 0
#With recursive transition probability = .9, given you are in state 1, you
#should wait until the context = -.5 with these values:
mu1 = -.3
mu2 = .3
sd1 = .37
sd2 = .37
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- 2*dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
x <- seq(from=-1, to=1,by=0.01)
curve(dnorm(x,-.3,.37), xlim=c(-1,1),
xlab = "Gray Spectrum", ylab = "Density")
segments(x, rep(0,length(x)),x,dnorm(x,-.3,.37) ,
col=grey.colors(length(x), start = 0, end = 1), lwd=2)
par(new = T)
curve(dnorm(x,-.3,.37), xlim=c(-1,1), ylab = "", xlab = "", lwd = 2)
curve(dnorm(x,-.3,.37), xlim=c(-1,1),
xlab = "Gray Spectrum", ylab = "Density")
segments(x, rep(0,length(x)),x,dnorm(x,-.3,.37) ,
col=grey.colors(length(x), start = 0, end = 1), lwd=2)
par(new = T)
curve(dnorm(x,.3,.37), xlim=c(-1,1), ylab = "", xlab = "")
segments(x, rep(0,length(x)),x,dnorm(x,.3,.37) ,
col=grey.colors(length(x), start = 0, end = 1), lwd=2)
par(new=T)
curve(dnorm(x,.3,.37), xlim=c(-1,1), ylab = "", xlab = "", lwd = 2)
par(new=T)
curve(dnorm(x,-.3,.37), xlim=c(-1,1), ylab = "", xlab = "", lwd = 2, lty = 3)
#show relative likelihood including a recursive transition probability of .9
#dashed line shows the likelihood given that one is in state 2 (mu = mu2)
#dotted line shows likelihood given one is in state 1
curve(choice_probabilities(x,.5,.5,1,2),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(choice_probabilities(x,.9,.1,1,2),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(choice_probabilities(x,.1,.9,1,2),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
?pf
?pn
?pnorm
pnorm(0, mean = -.3, sd = .37)
pnorm(-.3, mean = -.3, sd = .37)
pnorm(.37, mean = -.3, sd = .37)
pnorm(.1, mean = -.3, sd = .37)
pnorm(0, mean = -.3, sd = .37)
mu1 = -.3
mu2 = .3
sd1 = .37
sd2 = .37
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
mu1 = -.1
mu2 = .1
sd1 = .37
sd2 = .37
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
#Without hypothesis priors one should change their policy based on whether the
#context is > 0
#With recursive transition probability = .9, given you are in state 1, you
#should wait until the context = -.5 with these values:
mu1 = -.1
mu2 = .1
sd1 = .37
sd2 = .37
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
#Without hypothesis priors one should change their policy based on whether the
#context is > 0
#With recursive transition probability = .9, given you are in state 1, you
#should wait until the context = -.5 with these values:
mu1 = -.2
mu2 = .2
sd1 = .37
sd2 = .37
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
#graph the two distributions and their relative likelihood assuming a flat prior
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
#show relative likelihood including a recursive transition probability of .9
#dashed line shows the likelihood given that one is in state 2 (mu = mu2)
#dotted line shows likelihood given one is in state 1
relative_probabilities = function(x,prior1, prior2) {
dnorm(x,mean = mu1, sd = sd1)*prior1/
(dnorm(x,mean = mu2, sd = sd2)*prior2+
dnorm(x,mean = mu1, sd = sd1)*prior1)
}
curve(relative_probabilities(x,.5,.5),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(relative_probabilities(x,.9,.1),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(relative_probabilities(x,.1,.9),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
?dnorm
dnorm(0,-.3,.37)
dnorm(.1,-.3,.37)
dnorm(-/3,-.3,.37)
dnorm(-.3,-.3,.37)
pnorm(-.3,-.3,.37)
pnorm(0,-.3,.37)
pnorm(0,-.2,.37)
pnorm(0,-.2,.2)
pnorm(0,-.2,.37)
#Without hypothesis priors one should change their policy based on whether the
#context is > 0
#With recursive transition probability = .9, given you are in state 1, you
#should wait until the context = -.5 with these values:
mu1 = -.3
mu2 = .3
sd1 = .5
sd2 = .5
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
#Without hypothesis priors one should change their policy based on whether the
#context is > 0
#With recursive transition probability = .9, given you are in state 1, you
#should wait until the context = -.5 with these values:
mu1 = -.3
mu2 = .3
sd1 = .5
sd2 = .5
#calculate overlap of two normal distributions
min.f1f2 <- function(x, mu1, mu2, sd1, sd2) {
f1 <- dnorm(x, mean=mu1, sd=sd1)
f2 <- dnorm(x, mean=mu2, sd=sd2)
pmin(f1, f2)
}
integrate(min.f1f2, -Inf, Inf, mu1=mu1, mu2=mu2, sd1=sd1, sd2=sd2)
#graph the two distributions and their relative likelihood assuming a flat prior
curve(dnorm(x,mean = mu1, sd = sd1), -1, 1,
ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2), -1, 1,
yaxt = "n", ylab = "",ylim = c(0,2))
par(new = T)
curve(dnorm(x,mean = mu2, sd = sd2)/
(dnorm(x,mean = mu2, sd = sd2)+dnorm(x,mean = mu1, sd = sd1)),
-1, 1, yaxt = "n", ylab = "", ylim = c(0,2), col = 'red')
#show relative likelihood including a recursive transition probability of .9
#dashed line shows the likelihood given that one is in state 2 (mu = mu2)
#dotted line shows likelihood given one is in state 1
relative_probabilities = function(x,prior1, prior2) {
dnorm(x,mean = mu1, sd = sd1)*prior1/
(dnorm(x,mean = mu2, sd = sd2)*prior2+
dnorm(x,mean = mu1, sd = sd1)*prior1)
}
curve(relative_probabilities(x,.5,.5),-1,1,ylim=c(0,1),
ylab = "Posterior P(TS 1)", xlab = "Shade Context (Black to White)")
par(new = T)
curve(relative_probabilities(x,.9,.1),-1,1,ylim=c(0,1),
col = 'red',xlab = "", ylab = "")
par(new = T)
curve(relative_probabilities(x,.1,.9),-1,1,ylim=c(0,1), col = 'blue',
xlab = "", ylab = "")
title("Effect of prior")
legend("topright",  legend = c("P(TS 1) = .5","P(TS 1) = .9","P(TS 1) = .1"),
lty=c(1,1,1), col=c(1,2,4),ncol=1, cex = .8)
relative_probabilities(-.9,.9,.1)
relative_probabilities(-9,.9,.1)
relative_probabilities(.9,.9,.1)
pnorm(.9,.3,.5)
ls
dataFiles <- Sys.glob("../Data/*modeled.csv")
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
setwd("~/Box Sync/Programming/Experiments/Prob_Context_Task/Analysis")
dataFiles <- Sys.glob("../Data/*modeled.csv")
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
head(gtest_df)
dim(gtest_df)
ggplot(plot_df.query('rt>.1'), aes('opt_certainty', 'log_rt', color = 'id')) + geom_point() + geom_smooth(method = 'lm')
library(ggplot2)
library(plyr)
ggplot(plot_df, aes('opt_certainty', 'log_rt', color = 'id')) + geom_point() + geom_smooth(method = 'lm')
plot_df = gtest_learn_df
ggplot(plot_df, aes('opt_certainty', 'log_rt', color = 'id')) + geom_point() + geom_smooth(method = 'lm')
str(gtest_df)
gtest_df$id = factor(gtest_df$id)
g_learn_df$id = factor(gtest_learn_id)
ggplot(plot_df, aes('opt_certainty', 'log_rt')) + geom_point() + geom_smooth(method = 'lm')
ggplot(plot_df, aes('opt_certainty', 'rt')) + geom_point() + geom_smooth(method = 'lm')
head(gtest_learn_df)
ggplot(data = plot_df, aes('opt_certainty', 'rt')) + geom_point() + geom_smooth(method = 'lm')
ggplot(data = plot_df, aes('context', 'rt')) + geom_point() + geom_smooth(method = 'lm')
ggplot(data = plot_df, aes('context', 'rt')) + geom_point()
str(gtest_learn_df)
gtest_learn_df$id = factor(gtest_learn_id)
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
gtest_df$id = factor(gtest_df$id)
gtest_learn_df$id = factor(gtest_learn_id)
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
gtest_df$id = factor(gtest_df$id)
gtest_learn_df$id = factor(gtest_learn_df$id)
plot_df = gtest_learn_df
ggplot(data = plot_df, aes('context', 'rt')) + geom_point()
ggplot(data = plot_df, aes('context', 'rt')) + geom_point()
help ggplot
?ggplot
ggplot(data = plot_df, aes(context, log_rt)) + geom_point()
ggplot(data = plot_df, aes(opt_certainty, log_rt)) + geom_point()
ggplot(data = plot_df, aes(opt_certainty, log_rt)) + geom_point() + geom_smooth
ggplot(data = plot_df, aes(opt_certainty, log_rt)) + geom_point() + geom_smooth()
ggplot(data = plot_df, aes(opt_certainty, log_rt)) + geom_point() + geom_smooth(method = 'lm')
ggplot(data = plot_df, aes(opt_certainty, log_rt, color = id)) + geom_point() + geom_smooth(method = 'lm')
library(lme4)
lmer(log_rt ~ opt_certainty + (1|id,opt_certainty))
lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id))
lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_learn_df)
rs1 = lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_learn_df)
summary(rs1)
rs1 = lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_df)
summary(rs1)
rs1 = lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_learn_df)
summary(rs1)
rs1$
rs1?
attributes(rs1)
str(rs1)
ranef?
?ranef
ranef(rs1)
fixef(rs1)
ranef(rs1)
rs1 = lmer(log_rt ~ abs_context + (1 + abs_context|id), data = gtest_learn_df)
summary(rs1)
rs1 = lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_learn_df)
summary(rs1)
rs2 = lmer(log_rt ~ abs_context + (1 + abs_context|id), data = gtest_learn_df)
summary(rs2)
str(gtest_df)
glmer(subj_ts ~ opt_observer_posterior)
glmer(subj_ts ~ opt_observer_posterior, family = binomial)
glmer(subj_ts ~ opt_observer_posterior, family = binomial, data = gtest_learn_df)
glmer(subj_ts ~ opt_observer_posterior + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df)
glm(subj_ts ~ opt_observer_posterior + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df)
glm(subj_ts ~ opt_observer_posterior, family = binomial, data = gtest_learn_df)
summary(glmer(subj_ts ~ opt_observer_posterior + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
summary(glmer(subj_ts ~ midline_observer_posterior + opt_observer_posterior + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
str(gtest_df)
summary(glmer(subj_ts ~ midline_observer_choices + opt_observer_posterior + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
summary(glmer(subj_ts ~ midline_observer_choices + opt_observer_choices + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
library(ggplot2)
library(ggplot2)
library(plyr)
library(lme4)
gtest_df = read.csv('Analysis_Output/gtest_df.csv')
gtest_learn_df = read.csv('Analysis_Output/gtest_learn_df.csv')
gtest_df$id = factor(gtest_df$id)
gtest_learn_df$id = factor(gtest_learn_df$id)
head(gtest_learn_df)
str(gtest_learn_df)
#Plot rt against optimal model certainty
plot_df = gtest_learn_df
ggplot(data = plot_df, aes(opt_certainty, log_rt, color = id)) + geom_point() + geom_smooth(method = 'lm')
rs1 = lmer(log_rt ~ opt_certainty + (1 + opt_certainty|id), data = gtest_learn_df)
summary(rs1)
rs2 = lmer(log_rt ~ abs_context + (1 + abs_context|id), data = gtest_learn_df)
summary(rs2)
summary(glmer(subj_ts ~ midline_observer_choices + opt_observer_choices + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
str(gtest_learn_df)
plot_df = gtest_learn_df
ggplot(data = plot_df, aes(fit_certainty, log_rt, color = id)) + geom_point() + geom_smooth(method = 'lm')
rs1 = lmer(log_rt ~ fit_certainty + (1 + fit_certainty|id), data = gtest_learn_df)
summary(rs1)
rs2 = lmer(log_rt ~ abs_context + (1 + abs_context|id), data = gtest_learn_df)
summary(rs2)
summary(glmer(subj_ts ~ midline_observer_choices + opt_observer_choices + (1+opt_observer_posterior | id), family = binomial, data = gtest_learn_df))
```
rs3 = glmer(subj_ts ~ fit_observer_posterior + (1 + fit_observer_posterior | id), family = binomial, data = gtest_learn_df)
summary(rs3)
summary(glm(subj_ts ~ fit_observer_posterior , family = binomial, data = gtest_learn_df))
rs3 = glmer(subj_ts ~ fit_observer_posterior + (1 + fit_observer_posterior | id), family = binomial, data = gtest_learn_df)
summary(rs3)
rs3 = glmer(subj_ts ~ fit_observer_posterior + contexts + (1 + fit_observer_posterior | id), family = binomial, data = gtest_learn_df)
rs3 = glmer(subj_ts ~ fit_observer_posterior + context + (1 + fit_observer_posterior | id), family = binomial, data = gtest_learn_df)
summary(rs3)
lag(gtest_learn_df$context,1)
lag(gtest_learn_df$context,-1)
lag(gtest_learn_df$context,-2)
lag(gtest_learn_df$context,2)
summary(glm(subj_ts ~ fit_observer_posterior , family = binomial, data = gtest_learn_df))
summary(rs3)
gtest_learn_df$subj_ts-gtest_learn_df$fit_observer_posterior
(gtest_learn_df$fit_observer_posterior*(1-gtest_learn_df$fit_observer_posterior))
3^/5
3^.5
(gtest_learn_df$subj_ts-gtest_learn_df$fit_observer_posterior)/(gtest_learn_df$fit_observer_posterior*(1-gtest_learn_df$fit_observer_posterior))^.5
sum((gtest_learn_df$subj_ts-gtest_learn_df$fit_observer_posterior)/(gtest_learn_df$fit_observer_posterior*(1-gtest_learn_df$fit_observer_posterior))^.5)
chisq.test((gtest_learn_df$subj_ts-gtest_learn_df$fit_observer_posterior)/(gtest_learn_df$fit_observer_posterior*(1-gtest_learn_df$fit_observer_posterior))^.5)
chisq.test(((gtest_learn_df$subj_ts-gtest_learn_df$fit_observer_posterior)/(gtest_learn_df$fit_observer_posterior*(1-gtest_learn_df$fit_observer_posterior))^.5)^2)
